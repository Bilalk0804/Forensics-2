{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eb97a58d",
   "metadata": {},
   "source": [
    "# Sentinel Forensics — Kaggle GPU Inference Server\n",
    "\n",
    "This notebook runs the heavy ML models on Kaggle's free GPU (T4) and exposes them\n",
    "as an HTTP API via ngrok, so your local FastAPI backend can call them instead of\n",
    "loading models locally.\n",
    "\n",
    "## Setup steps\n",
    "1. **Enable GPU**: Notebook settings → Accelerator → **GPU T4 x2**\n",
    "2. **Add ngrok token**: Notebook settings → Secrets → Add `NGROK_TOKEN`\n",
    "   - Get a free token at https://ngrok.com (sign up → Your Authtoken)\n",
    "3. **Run all cells** (`Run All`)\n",
    "4. **Copy the ngrok URL** printed in the last cell output\n",
    "5. In your local terminal set:\n",
    "   ```\n",
    "   set USE_REMOTE_INFERENCE=true\n",
    "   set KAGGLE_INFERENCE_URL=https://xxxx.ngrok-free.app\n",
    "   ```\n",
    "6. Run `python run_api.py` locally — startup in ~5 seconds!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94a04605",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1 — Install dependencies\n",
    "!pip install fastapi uvicorn pyngrok transformers torch torchvision torchaudio \\\n",
    "             ultralytics httpx Pillow opencv-python-headless -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcc0267e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2 — Load all 4 GPU models\n",
    "import os, logging, base64, io, tempfile, threading\n",
    "import torch\n",
    "\n",
    "os.environ.setdefault('USE_TORCH', '1')\n",
    "os.environ.setdefault('TRANSFORMERS_NO_TF', '1')\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s [%(levelname)s] %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "DEVICE = 0 if torch.cuda.is_available() else -1\n",
    "DEVICE_STR = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "print(f'Device: {DEVICE_STR}')\n",
    "\n",
    "# ── Text models ─────────────────────────────────────────────────────────────\n",
    "from transformers import pipeline as hf_pipeline, AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "THREAT_CATEGORIES = [\n",
    "    'financial fraud', 'violence or threats', 'drug trafficking',\n",
    "    'cyber crime', 'identity theft', 'exploitation',\n",
    "    'corruption', 'weapons', 'general / benign',\n",
    "]\n",
    "\n",
    "print('Loading NER...')\n",
    "ner_pipeline = hf_pipeline('ner', model='dslim/bert-base-NER',\n",
    "                            aggregation_strategy='simple', device=DEVICE)\n",
    "\n",
    "print('Loading summarizer...')\n",
    "_sum_tok = AutoTokenizer.from_pretrained('sshleifer/distilbart-cnn-12-6')\n",
    "_sum_mod = AutoModelForSeq2SeqLM.from_pretrained('sshleifer/distilbart-cnn-12-6')\n",
    "if DEVICE >= 0:\n",
    "    _sum_mod = _sum_mod.to(DEVICE_STR)\n",
    "\n",
    "print('Loading zero-shot classifier...')\n",
    "classifier_pipeline = hf_pipeline('zero-shot-classification',\n",
    "                                   model='facebook/bart-large-mnli', device=DEVICE)\n",
    "print('✓ Text models ready')\n",
    "\n",
    "# ── Audio model ─────────────────────────────────────────────────────────────\n",
    "print('Loading Whisper...')\n",
    "whisper_pipeline = hf_pipeline('automatic-speech-recognition',\n",
    "                                model='openai/whisper-tiny', device=DEVICE)\n",
    "print('✓ Audio model ready')\n",
    "\n",
    "# ── Vision models ────────────────────────────────────────────────────────────\n",
    "from ultralytics import YOLO\n",
    "from transformers import ViTForImageClassification, ViTImageProcessor\n",
    "\n",
    "print('Loading YOLOv8n...')\n",
    "yolo_model = YOLO('yolov8n.pt')\n",
    "\n",
    "print('Loading violence detector...')\n",
    "_vit_name = 'jaranohaal/vit-base-violence-detection'\n",
    "vit_processor = ViTImageProcessor(size=224, image_mean=[0.5,0.5,0.5], image_std=[0.5,0.5,0.5])\n",
    "vit_model = ViTForImageClassification.from_pretrained(_vit_name)\n",
    "if DEVICE >= 0:\n",
    "    vit_model = vit_model.to(DEVICE_STR)\n",
    "vit_model.eval()\n",
    "print('✓ Vision models ready')\n",
    "\n",
    "# ── Deepfake model ───────────────────────────────────────────────────────────\n",
    "from transformers import SiglipForImageClassification, AutoImageProcessor\n",
    "\n",
    "print('Loading deepfake detector...')\n",
    "_df_name = 'prithivMLmods/deepfake-detector-model-v1'\n",
    "df_processor = AutoImageProcessor.from_pretrained(_df_name)\n",
    "df_model = SiglipForImageClassification.from_pretrained(_df_name)\n",
    "df_model.eval()\n",
    "print('✓ Deepfake model ready')\n",
    "\n",
    "print('\\n=== All models loaded ===')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1dbd5c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3 — Inference helpers\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "# ── Text inference ───────────────────────────────────────────────────────────\n",
    "def infer_text(text: str) -> dict:\n",
    "    results = {}\n",
    "    # NER\n",
    "    try:\n",
    "        raw = ner_pipeline(text[:512])\n",
    "        entities = [{'text': e['word'], 'entity_type': e['entity_group'],\n",
    "                     'start': e['start'], 'end': e['end'],\n",
    "                     'confidence': round(float(e['score']), 4)} for e in raw]\n",
    "        results['ner'] = 'loaded'\n",
    "    except Exception as e:\n",
    "        entities = []; results['ner'] = f'error: {e}'\n",
    "\n",
    "    # Summarization\n",
    "    try:\n",
    "        inputs = _sum_tok(text, return_tensors='pt', max_length=1024,\n",
    "                          truncation=True)\n",
    "        if DEVICE >= 0:\n",
    "            inputs = {k: v.to(DEVICE_STR) for k, v in inputs.items()}\n",
    "        with torch.no_grad():\n",
    "            out = _sum_mod.generate(**inputs, max_new_tokens=128)\n",
    "        summary = _sum_tok.decode(out[0], skip_special_tokens=True)\n",
    "        results['summarizer'] = 'loaded'\n",
    "    except Exception as e:\n",
    "        summary = ''; results['summarizer'] = f'error: {e}'\n",
    "\n",
    "    # Zero-shot classification\n",
    "    top_label, top_score, categories = 'general / benign', 0.0, []\n",
    "    try:\n",
    "        clf_out = classifier_pipeline(text[:1024], THREAT_CATEGORIES)\n",
    "        categories = [{'label': l, 'score': round(s, 4)}\n",
    "                      for l, s in zip(clf_out['labels'], clf_out['scores'])]\n",
    "        top_label = clf_out['labels'][0]\n",
    "        top_score = round(float(clf_out['scores'][0]), 4)\n",
    "        results['classifier'] = 'loaded'\n",
    "    except Exception as e:\n",
    "        results['classifier'] = f'error: {e}'\n",
    "\n",
    "    risk_level = 'HIGH' if top_score > 0.7 else ('MEDIUM' if top_score > 0.4 else 'LOW')\n",
    "    if top_label == 'general / benign':\n",
    "        risk_level = 'LOW'\n",
    "\n",
    "    return {\n",
    "        'label': top_label,\n",
    "        'confidence': top_score,\n",
    "        'tokens_processed': len(text.split()),\n",
    "        'entities': entities,\n",
    "        'summary': summary,\n",
    "        'categories': categories,\n",
    "        'risk_level': risk_level,\n",
    "        'components_status': results,\n",
    "    }\n",
    "\n",
    "\n",
    "# ── Audio inference ──────────────────────────────────────────────────────────\n",
    "def infer_audio(data: bytes) -> dict:\n",
    "    try:\n",
    "        with tempfile.NamedTemporaryFile(suffix='.wav', delete=False) as f:\n",
    "            f.write(data); tmp = f.name\n",
    "        result = whisper_pipeline(tmp)\n",
    "        os.unlink(tmp)\n",
    "        transcription = result.get('text', '').strip()\n",
    "        has_speech = bool(transcription)\n",
    "        return {\n",
    "            'label': 'speech-detected' if has_speech else 'no-speech',\n",
    "            'confidence': 0.9 if has_speech else 0.5,\n",
    "            'transcription': transcription,\n",
    "            'duration_seconds': 0.0,\n",
    "        }\n",
    "    except Exception as e:\n",
    "        return {'label': 'error', 'confidence': 0.0,\n",
    "                'transcription': '', 'duration_seconds': 0.0, 'error': str(e)}\n",
    "\n",
    "\n",
    "# ── Vision inference ─────────────────────────────────────────────────────────\n",
    "def infer_vision(data: bytes) -> dict:\n",
    "    try:\n",
    "        img = Image.open(io.BytesIO(data)).convert('RGB')\n",
    "        img_arr = np.array(img)\n",
    "\n",
    "        # YOLO detection\n",
    "        yolo_results = yolo_model(img_arr, device='cpu', verbose=False)\n",
    "        detections = []\n",
    "        for r in yolo_results:\n",
    "            for box in r.boxes:\n",
    "                detections.append({\n",
    "                    'class_name': r.names[int(box.cls)],\n",
    "                    'confidence': round(float(box.conf), 4),\n",
    "                    'bbox': [round(float(x), 1) for x in box.xyxy[0].tolist()],\n",
    "                })\n",
    "\n",
    "        # Violence detection\n",
    "        inputs = vit_processor(images=img, return_tensors='pt')\n",
    "        if DEVICE >= 0:\n",
    "            inputs = {k: v.to(DEVICE_STR) for k, v in inputs.items()}\n",
    "        with torch.no_grad():\n",
    "            logits = vit_model(**inputs).logits\n",
    "        probs = torch.softmax(logits, dim=-1).squeeze()\n",
    "        violence_idx = list(vit_model.config.id2label.values()).index('Violence') \\\n",
    "                       if 'Violence' in vit_model.config.id2label.values() else 0\n",
    "        violence_score = float(probs[violence_idx])\n",
    "        violence_detected = violence_score > 0.5\n",
    "\n",
    "        top_label = detections[0]['class_name'] if detections else \\\n",
    "                    ('violence-detected' if violence_detected else 'no-objects')\n",
    "        top_conf = detections[0]['confidence'] if detections else violence_score\n",
    "        risk = 'HIGH' if violence_detected or top_conf > 0.8 else \\\n",
    "               ('MEDIUM' if top_conf > 0.5 else 'LOW')\n",
    "\n",
    "        return {\n",
    "            'label': top_label, 'confidence': round(top_conf, 4),\n",
    "            'detections': detections, 'violence_detected': violence_detected,\n",
    "            'violence_score': round(violence_score, 4),\n",
    "            'risk_level': risk, 'detection_count': len(detections),\n",
    "        }\n",
    "    except Exception as e:\n",
    "        return {'label': 'error', 'confidence': 0.0, 'detections': [],\n",
    "                'violence_detected': False, 'violence_score': 0.0,\n",
    "                'risk_level': 'LOW', 'detection_count': 0, 'error': str(e)}\n",
    "\n",
    "\n",
    "# ── Deepfake inference ────────────────────────────────────────────────────────\n",
    "def _classify_frame(frame_img: Image.Image) -> tuple[bool, float, str]:\n",
    "    inputs = df_processor(images=frame_img, return_tensors='pt')\n",
    "    with torch.no_grad():\n",
    "        logits = df_model(**inputs).logits\n",
    "    probs = torch.softmax(logits, dim=-1).squeeze()\n",
    "    pred_idx = int(torch.argmax(probs))\n",
    "    label = df_model.config.id2label[pred_idx].lower()\n",
    "    conf = float(probs[pred_idx])\n",
    "    return ('fake' in label), conf, label\n",
    "\n",
    "def infer_deepfake(data: bytes) -> dict:\n",
    "    try:\n",
    "        img = Image.open(io.BytesIO(data)).convert('RGB')\n",
    "        is_fake, conf, label = _classify_frame(img)\n",
    "        return {'is_deepfake': is_fake, 'confidence': round(conf, 4), 'label': label}\n",
    "    except Exception as e:\n",
    "        return {'is_deepfake': False, 'confidence': 0.0,\n",
    "                'label': 'inference-error', 'error': str(e)}\n",
    "\n",
    "print('✓ Inference helpers ready')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea7cfdf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4 — FastAPI app (async endpoints — handles concurrent requests)\n",
    "from fastapi import FastAPI\n",
    "from pydantic import BaseModel\n",
    "import uvicorn\n",
    "import asyncio\n",
    "\n",
    "app = FastAPI(title='Sentinel Forensics — Kaggle GPU Inference Server')\n",
    "\n",
    "class TextRequest(BaseModel):\n",
    "    text: str\n",
    "\n",
    "class BytesRequest(BaseModel):\n",
    "    data_b64: str  # base64-encoded bytes\n",
    "\n",
    "@app.get('/health')\n",
    "def health():\n",
    "    return {'status': 'ok', 'device': DEVICE_STR,\n",
    "            'models': ['text', 'audio', 'vision', 'deepfake']}\n",
    "\n",
    "# Async endpoints run inference in a thread pool so FastAPI can accept\n",
    "# new requests while another is being processed on GPU.\n",
    "@app.post('/predict/text')\n",
    "async def predict_text(req: TextRequest):\n",
    "    return await asyncio.to_thread(infer_text, req.text)\n",
    "\n",
    "@app.post('/predict/audio')\n",
    "async def predict_audio(req: BytesRequest):\n",
    "    data = base64.b64decode(req.data_b64)\n",
    "    return await asyncio.to_thread(infer_audio, data)\n",
    "\n",
    "@app.post('/predict/vision')\n",
    "async def predict_vision(req: BytesRequest):\n",
    "    data = base64.b64decode(req.data_b64)\n",
    "    return await asyncio.to_thread(infer_vision, data)\n",
    "\n",
    "@app.post('/predict/deepfake')\n",
    "async def predict_deepfake(req: BytesRequest):\n",
    "    data = base64.b64decode(req.data_b64)\n",
    "    return await asyncio.to_thread(infer_deepfake, data)\n",
    "\n",
    "print('✓ FastAPI app created (async endpoints)')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba74abe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5 — Start server and expose via ngrok\n",
    "from pyngrok import ngrok\n",
    "import threading, time\n",
    "\n",
    "# ── Get ngrok token ──────────────────────────────────────────────────────────\n",
    "NGROK_TOKEN_FALLBACK = \"\"  # ← paste your ngrok token here if secret doesn't work\n",
    "\n",
    "ngrok_token = None\n",
    "try:\n",
    "    from kaggle_secrets import UserSecretsClient\n",
    "    secrets = UserSecretsClient()\n",
    "    ngrok_token = secrets.get_secret('NGROK_TOKEN')\n",
    "    print(\"✓ Loaded ngrok token from Kaggle secret\")\n",
    "except Exception as e:\n",
    "    print(f\"⚠ Could not load Kaggle secret: {e}\")\n",
    "    if NGROK_TOKEN_FALLBACK:\n",
    "        ngrok_token = NGROK_TOKEN_FALLBACK\n",
    "        print(\"✓ Using fallback token from NGROK_TOKEN_FALLBACK\")\n",
    "    else:\n",
    "        raise RuntimeError(\n",
    "            \"No ngrok token found!\\n\"\n",
    "            \"Fix option 1: Kaggle → Add-ons → Secrets → toggle NGROK_TOKEN ON\\n\"\n",
    "            \"Fix option 2: Paste your token in NGROK_TOKEN_FALLBACK above\"\n",
    "        )\n",
    "\n",
    "ngrok.set_auth_token(ngrok_token)\n",
    "\n",
    "# ── Start FastAPI with thread pool — handles concurrent requests properly ────\n",
    "# run_in_threadpool=True lets multiple requests run simultaneously\n",
    "# even though Python has the GIL, I/O and GPU ops release it\n",
    "def run_server():\n",
    "    uvicorn.run(\n",
    "        app,\n",
    "        host='0.0.0.0',\n",
    "        port=8000,\n",
    "        log_level='warning',\n",
    "        workers=1,           # single process (models are in-memory)\n",
    "        loop='asyncio',\n",
    "    )\n",
    "\n",
    "thread = threading.Thread(target=run_server, daemon=True)\n",
    "thread.start()\n",
    "time.sleep(2)\n",
    "\n",
    "# ── Open ngrok tunnel ─────────────────────────────────────────────────────────\n",
    "tunnel = ngrok.connect(8000)\n",
    "public_url = tunnel.public_url\n",
    "\n",
    "print('=' * 60)\n",
    "print('KAGGLE GPU INFERENCE SERVER IS RUNNING')\n",
    "print('=' * 60)\n",
    "print(f'Public URL: {public_url}')\n",
    "print()\n",
    "print('Copy the URL above, then in your LOCAL terminal run:')\n",
    "print(f'  set USE_REMOTE_INFERENCE=true')\n",
    "print(f'  set KAGGLE_INFERENCE_URL={public_url}')\n",
    "print(f'  python run_api.py')\n",
    "print('=' * 60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5fabeb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6 — Keep-alive (run this to keep the session alive)\n",
    "# Kaggle sessions stay active as long as a cell is running.\n",
    "# This cell loops and prints a heartbeat every 5 minutes.\n",
    "import time\n",
    "print('Keep-alive started. Press Stop to shut down the server.')\n",
    "i = 0\n",
    "while True:\n",
    "    time.sleep(300)\n",
    "    i += 1\n",
    "    print(f'[heartbeat {i}] Server still running at {public_url}')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
